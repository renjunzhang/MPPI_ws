#!/usr/bin/env python3
"""
æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å·¥å…·
================

åŠŸèƒ½:
    1. è®¢é˜… /nav_metrics å’Œ /nav_algo_name
    2. æ”¶é›†ä¸‰ç§ç®—æ³• (MPC, MPPI, iLQR) çš„ç»“æœ
    3. æ”¶é›†å®Œæ¯•åè‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    4. æ”¯æŒä¿å­˜åˆ° CSV

ä½¿ç”¨æ–¹å¼:
    1. åœ¨ç»ˆç«¯1è¿è¡Œ: rosrun simple_mppi plot_metrics.py
    2. åœ¨ç»ˆç«¯2åˆ†åˆ«è¿è¡Œä¸‰ä¸ª launch æ–‡ä»¶
    3. æ¯æ¬¡å‘å¸ƒç›®æ ‡ç‚¹ (0.67, 0.65) åç­‰å¾…å¯¼èˆªå®Œæˆ
    4. æ”¶é›†åˆ°ä¸‰ä¸ªç»“æœåè‡ªåŠ¨è¾“å‡ºå¯¹æ¯”

æŒ‡æ ‡æ ¼å¼ (Float32MultiArray, 16 å­—æ®µ):
    [path_length, avg_speed, avg_cte, max_cte,
     jerk_v, jerk_w, v_variance, w_variance, control_effort,
     total_time, efficiency, avg_solve_time,
     heading_error, oscillation_count, stuck_time, success]
"""
import rospy
import numpy as np
from std_msgs.msg import Float32MultiArray, String
from datetime import datetime


class MetricsComparator:
    """ä¸‰ç§æ¶æ„æ€§èƒ½å¯¹æ¯”å™¨"""

    # æŒ‡æ ‡åç§°æ˜ å°„
    METRIC_NAMES = [
        'path_length', 'avg_speed', 'avg_cte', 'max_cte',
        'jerk_v', 'jerk_w', 'v_variance', 'w_variance', 'control_effort',
        'total_time', 'efficiency', 'avg_solve_time',
        'heading_error', 'oscillation_count', 'stuck_time', 'success'
    ]

    # æœŸæœ›æ”¶é›†çš„ç®—æ³•
    EXPECTED_ALGOS = {'MPC', 'MPPI', 'iLQR'}

    def __init__(self):
        rospy.init_node('metrics_comparator')
        
        self.records = {}  # {algo_name: metrics_dict}
        self.current_algo = None
        
        # è®¢é˜…
        rospy.Subscriber('/nav_algo_name', String, self._algo_cb)
        rospy.Subscriber('/nav_metrics', Float32MultiArray, self._metrics_cb)
        
        rospy.loginfo("=" * 50)
        rospy.loginfo("  æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å·¥å…·")
        rospy.loginfo("=" * 50)
        rospy.loginfo("  ç­‰å¾…æ”¶é›†ä¸‰ç§ç®—æ³•çš„ç»“æœ...")
        rospy.loginfo("  å·²æ”¶é›†: 0/3")
        rospy.loginfo("=" * 50)

    def _algo_cb(self, msg):
        """æ¥æ”¶ç®—æ³•åç§°"""
        self.current_algo = msg.data
        rospy.loginfo(f"[Comparator] æ¥æ”¶åˆ°ç®—æ³•: {self.current_algo}")

    def _metrics_cb(self, msg):
        """æ¥æ”¶æŒ‡æ ‡æ•°æ®"""
        if self.current_algo is None:
            rospy.logwarn("[Comparator] æœªæ”¶åˆ°ç®—æ³•åç§°ï¼Œå¿½ç•¥æ­¤æ¬¡æ•°æ®")
            return
        
        if len(msg.data) < 16:
            rospy.logwarn(f"[Comparator] æ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ› 16 å­—æ®µï¼Œæ”¶åˆ° {len(msg.data)}")
            return
        
        # è§£ææŒ‡æ ‡
        metrics = {}
        for i, name in enumerate(self.METRIC_NAMES):
            metrics[name] = msg.data[i]
        
        # å­˜å‚¨
        self.records[self.current_algo] = metrics
        
        rospy.loginfo(f"[Comparator] å·²è®°å½• {self.current_algo} çš„ç»“æœ")
        rospy.loginfo(f"[Comparator] å·²æ”¶é›†: {len(self.records)}/3 ({', '.join(self.records.keys())})")
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†å®Œæ¯•
        if self._check_complete():
            self._print_comparison()
            self._save_csv()

    def _check_complete(self):
        """æ£€æŸ¥æ˜¯å¦æ”¶é›†å®Œä¸‰ç§ç®—æ³•"""
        return self.EXPECTED_ALGOS.issubset(set(self.records.keys()))

    def _print_comparison(self):
        """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""
        print("\n")
        print("=" * 80)
        print("                    ä¸‰ç§æ¶æ„æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("=" * 80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ç›®æ ‡: (0.67, 0.65)")
        
        # === æˆåŠŸç‡ ===
        print("\n" + "â”€" * 80)
        print("  âœ“ æˆåŠŸçŠ¶æ€")
        print("â”€" * 80)
        for algo in ['MPC', 'MPPI', 'iLQR']:
            success = self.records[algo]['success']
            status = "âœ… æˆåŠŸ" if success > 0.5 else "âŒ å¤±è´¥"
            print(f"  {algo:8} {status}")
        
        # === æ•ˆç‡æŒ‡æ ‡ ===
        print("\n" + "â”€" * 80)
        print("  ğŸ“Š æ•ˆç‡æŒ‡æ ‡")
        print("â”€" * 80)
        self._print_metric_row('è·¯å¾„é•¿åº¦ (m)', 'path_length', '{:.3f}', lower_better=True)
        self._print_metric_row('æ€»æ—¶é—´ (s)', 'total_time', '{:.2f}', lower_better=True)
        self._print_metric_row('å¹³å‡é€Ÿåº¦ (m/s)', 'avg_speed', '{:.3f}', lower_better=False)
        self._print_metric_row('è·¯å¾„æ•ˆç‡ (%)', 'efficiency', '{:.1f}', lower_better=False, scale=100)
        
        # === è·Ÿè¸ªè´¨é‡ ===
        print("\n" + "â”€" * 80)
        print("  ğŸ¯ è·Ÿè¸ªè´¨é‡")
        print("â”€" * 80)
        self._print_metric_row('å¹³å‡ CTE (cm)', 'avg_cte', '{:.2f}', lower_better=True, scale=100)
        self._print_metric_row('æœ€å¤§ CTE (cm)', 'max_cte', '{:.2f}', lower_better=True, scale=100)
        self._print_metric_row('æœå‘è¯¯å·® (Â°)', 'heading_error', '{:.1f}', lower_better=True, scale=57.3)
        
        # === æ§åˆ¶å¹³æ»‘æ€§ ===
        print("\n" + "â”€" * 80)
        print("  ğŸ›ï¸  æ§åˆ¶å¹³æ»‘æ€§")
        print("â”€" * 80)
        self._print_metric_row('é€Ÿåº¦æ–¹å·®', 'v_variance', '{:.4f}', lower_better=True)
        self._print_metric_row('è§’é€Ÿåº¦æ–¹å·®', 'w_variance', '{:.4f}', lower_better=True)
        self._print_metric_row('Jerk(v)', 'jerk_v', '{:.4f}', lower_better=True)
        self._print_metric_row('Jerk(w)', 'jerk_w', '{:.4f}', lower_better=True)
        self._print_metric_row('éœ‡è¡æ¬¡æ•°', 'oscillation_count', '{:.0f}', lower_better=True)
        
        # === è®¡ç®—æ•ˆç‡ ===
        print("\n" + "â”€" * 80)
        print("  âš¡ è®¡ç®—æ•ˆç‡")
        print("â”€" * 80)
        self._print_metric_row('å¹³å‡æ±‚è§£ (ms)', 'avg_solve_time', '{:.2f}', lower_better=True)
        self._print_metric_row('å¡ä½æ—¶é—´ (s)', 'stuck_time', '{:.2f}', lower_better=True)
        
        # === ç»¼åˆè¯„åˆ† ===
        print("\n" + "â”€" * 80)
        print("  ğŸ† ç»¼åˆè¯„åˆ† (è¶Šé«˜è¶Šå¥½)")
        print("â”€" * 80)
        scores = self._compute_scores()
        max_score = max(scores.values())
        for algo in ['MPC', 'MPPI', 'iLQR']:
            score = scores[algo]
            medal = "ğŸ¥‡" if score == max_score else "  "
            bar = "â–ˆ" * int(score / 10) + "â–‘" * (10 - int(score / 10))
            print(f"  {medal} {algo:8} {bar} {score:.1f}")
        
        print("\n" + "=" * 80)
        print("                         å¯¹æ¯”å®Œæˆï¼")
        print("=" * 80 + "\n")

    def _print_metric_row(self, label, key, fmt, lower_better=True, scale=1.0):
        """æ‰“å°ä¸€è¡ŒæŒ‡æ ‡å¯¹æ¯”"""
        values = {}
        for algo in ['MPC', 'MPPI', 'iLQR']:
            values[algo] = self.records[algo][key] * scale
        
        # æ‰¾æœ€ä¼˜å€¼
        if lower_better:
            best_val = min(values.values())
        else:
            best_val = max(values.values())
        
        # æ ¼å¼åŒ–è¾“å‡º
        line = f"  {label:16}"
        for algo in ['MPC', 'MPPI', 'iLQR']:
            val = values[algo]
            formatted = fmt.format(val)
            if abs(val - best_val) < 1e-6:
                line += f"  {algo}: {formatted:>10} ğŸ†"
            else:
                line += f"  {algo}: {formatted:>10}   "
        print(line)

    def _compute_scores(self):
        """è®¡ç®—ç»¼åˆè¯„åˆ† (0-100)"""
        scores = {}
        
        for algo in ['MPC', 'MPPI', 'iLQR']:
            m = self.records[algo]
            
            # å„é¡¹è¯„åˆ† (å½’ä¸€åŒ–åˆ° 0-100)
            # è·Ÿè¸ªè´¨é‡ (40%)
            cte_score = max(0, 100 - m['avg_cte'] * 1000)  # CTE è¶Šå°è¶Šå¥½
            heading_score = max(0, 100 - np.degrees(m['heading_error']) * 5)
            tracking = (cte_score * 0.6 + heading_score * 0.4) * 0.4
            
            # æ•ˆç‡ (30%)
            efficiency_score = m['efficiency'] * 100
            time_score = max(0, 100 - m['total_time'] * 2)
            efficiency = (efficiency_score * 0.6 + time_score * 0.4) * 0.3
            
            # å¹³æ»‘æ€§ (20%)
            osc_score = max(0, 100 - m['oscillation_count'] * 2)
            var_score = max(0, 100 - m['w_variance'] * 500)
            smoothness = (osc_score * 0.5 + var_score * 0.5) * 0.2
            
            # è®¡ç®—æ•ˆç‡ (10%)
            solve_score = max(0, 100 - m['avg_solve_time'] * 2)
            compute = solve_score * 0.1
            
            scores[algo] = tracking + efficiency + smoothness + compute
        
        return scores

    def _save_csv(self):
        """ä¿å­˜ç»“æœåˆ° CSV"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"/tmp/nav_comparison_{timestamp}.csv"
        
        with open(filename, 'w') as f:
            # è¡¨å¤´
            f.write("Metric," + ",".join(['MPC', 'MPPI', 'iLQR']) + "\n")
            
            # æ•°æ®è¡Œ
            for name in self.METRIC_NAMES:
                values = [str(self.records[algo][name]) for algo in ['MPC', 'MPPI', 'iLQR']]
                f.write(f"{name},{','.join(values)}\n")
        
        rospy.loginfo(f"[Comparator] ç»“æœå·²ä¿å­˜åˆ°: {filename}")

    def run(self):
        """è¿è¡Œ"""
        rospy.spin()


def main():
    try:
        comparator = MetricsComparator()
        comparator.run()
    except rospy.ROSInterruptException:
        pass


if __name__ == '__main__':
    main()
